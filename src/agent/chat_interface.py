"""
Chat Interface Module - Interfaz de usuario para interacci√≥n conversacional

Responsabilidad: Interfaz de usuario para interacci√≥n conversacional
- Captura de consultas del usuario
- Visualizaci√≥n de respuestas formateadas
- Manejo de historial de conversaci√≥n
- Soporte futuro para streaming progresivo
"""

import logging
import sys
from typing import Optional, Dict, Any
from datetime import datetime
import uuid
import traceback

from request_handler import RequestHandler, RequestResult, RequestState
from conversation_logger import ConversationLogger
from llm_communication import LLMCommunication, LLMRequest
from tool_executor import ToolExecutor
from streaming_state_machine import StreamingStateMachine
from streaming_display import StreamingDisplay
from config_manager import ConfigManager
from session_manager import SessionManager
from color_utils import (
    user_text, llm_response, tool_result, info, warning, error, success,
    header, dim_text, format_user_input_section, format_metrics_section
)


class ChatInterface:
    """Interfaz de chat para interacci√≥n con el agente"""
    
    def __init__(self, config_path: str = "config/config_darwin.yaml", app_name: str = "Darwin", enable_streaming: bool = True, session_manager: Optional[SessionManager] = None):
        """
        Inicializa la interfaz de chat
        
        Args:
            config_path: Ruta al archivo de configuraci√≥n
            app_name: Nombre de la aplicaci√≥n (Darwin, SAP, MuleSoft, etc.)
            enable_streaming: Si debe usar streaming para respuestas del LLM
            session_manager: Gestor de sesiones (opcional, se crea uno nuevo si no se proporciona)
        """
        self.logger = logging.getLogger(__name__)
        self.config_path = config_path
        self.app_name = app_name
        self.enable_streaming = enable_streaming
        
        # Cargar configuraci√≥n
        self.config_manager = ConfigManager(config_path=config_path)
        
        # Inicializar manejador de requests (sin pasar system_prompt, se cargar√° desde configuraci√≥n)
        self.request_handler = RequestHandler(config_path=config_path)
        
        # Gestor de sesiones
        self.session_manager = session_manager or SessionManager()
        
        # Obtener sesi√≥n actual del gestor
        current_session = self.session_manager.get_current_session()
        if current_session:
            self.session_id = current_session.session_id
            self.logger.info(f"Usando sesi√≥n existente: {self.session_id}")
        else:
            # Fallback: crear sesi√≥n temporal si no hay una activa
            self.session_id = str(uuid.uuid4())
            self.logger.warning(f"No hay sesi√≥n activa, usando ID temporal: {self.session_id}")
        
        # Inicializar logger de conversaciones con gestor de sesiones
        self.conversation_logger = ConversationLogger(
            logs_dir="logs",
            session_manager=self.session_manager
        )
        
        # Inicializar componentes de streaming
        if enable_streaming:
            self.llm_comm = LLMCommunication(config_path=config_path)
            self.tool_executor = ToolExecutor(config_path=config_path, app_name=app_name.lower())
            self.logger.info("Streaming habilitado")
        
        self.is_active = False
        
        self.logger.info(f"ChatInterface inicializado con sesi√≥n {self.session_id}")
        self.logger.info(f"Logs de conversaci√≥n se guardar√°n en: {self.conversation_logger.get_log_path()}")
    
    def start(self) -> None:
        """Inicia la interfaz de chat"""
        self.is_active = True
        self._display_welcome_message()
    
    def stop(self) -> None:
        """Detiene la interfaz de chat"""
        self.is_active = False
        self.request_handler.end_session(self.session_id)
        
        # Finalizar sesi√≥n en el gestor
        self.session_manager.end_session()
        
        self._display_goodbye_message()
    
    def _display_welcome_message(self) -> None:
        """Muestra mensaje de bienvenida"""
        # Calcular espacios para alinear correctamente
        # La l√≠nea tiene 64 caracteres entre ‚ïë y ‚ïë
        # "  Bienvenido al asistente especializado en consultas sobre      " = 64 chars
        # "  la base de conocimiento " = 27 chars, dejando 37 chars para app_name + espacios
        app_name_line = f"  la base de conocimiento {self.app_name}"
        spaces_needed = 64 - len(app_name_line)
        app_name_formatted = app_name_line + " " * spaces_needed
        
        welcome = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                ‚ïë
‚ïë        AGENTE IA DE CONSULTA - {self.app_name.upper():^30}  ‚ïë
‚ïë                                                                ‚ïë
‚ïë  Bienvenido al asistente especializado en consultas sobre      ‚ïë
‚ïë{app_name_formatted}‚ïë
‚ïë                                                                ‚ïë
‚ïë  Puedo ayudarte con:                                           ‚ïë
‚ïë  ‚Ä¢ Consultas funcionales sobre el sistema                      ‚ïë
‚ïë  ‚Ä¢ Consultas t√©cnicas sobre implementaci√≥n                     ‚ïë
‚ïë  ‚Ä¢ B√∫squedas en la documentaci√≥n                               ‚ïë
‚ïë                                                                ‚ïë
‚ïë  Escribe 'salir' para terminar la sesi√≥n                       ‚ïë
‚ïë  Escribe 'historial' para ver el historial de conversaci√≥n     ‚ïë
‚ïë  Escribe 'estad√≠sticas' para ver estad√≠sticas de la sesi√≥n     ‚ïë
‚ïë                                                                ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
        print(welcome)
    
    def _display_goodbye_message(self) -> None:
        """Muestra mensaje de despedida"""
        goodbye = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                ‚ïë
‚ïë  ¬°Gracias por usar el Agente IA de Consulta {self.app_name}!{' '*(18-len(self.app_name))}‚ïë
‚ïë                                                                ‚ïë
‚ïë  Sesi√≥n finalizada correctamente.                              ‚ïë
‚ïë                                                                ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
        print(goodbye)
    
    def _format_tool_results_for_llm(self, tool_results, original_question: str = None) -> str:
        """
        Formatea los resultados de las herramientas para enviarlos al LLM
        
        Args:
            tool_results: Lista de ToolResult
            original_question: Pregunta original del usuario
            
        Returns:
            String formateado con los resultados
        """
        message = "[RESULTADOS DE TUS HERRAMIENTAS]\n\n"
        
        if original_question:
            message += f"RECORDATORIO - Pregunta original del usuario: \"{original_question}\"\n\n"
        
        message += "IMPORTANTE: Analiza estos resultados y presenta tu respuesta al usuario usando <present_answer>.\n"
        message += "NO solicites m√°s herramientas a menos que la informaci√≥n sea claramente insuficiente.\n\n"
        
        for result in tool_results:
            tool_name = result.tool_type.value if hasattr(result.tool_type, 'value') else str(result.tool_type)

            if result.success and result.data is not None:
                # Formatear seg√∫n el tipo de herramienta
                if tool_name in ["semantic_search", "lexical_search", "regex_search"]:
                    message += self._format_search_results(result.data)
                elif tool_name == "get_file_content":
                    message += self._format_file_content(result.data)
                else:
                    message += f"{str(result.data)}\n\n"
            elif not result.success:
                message += f"Error en {tool_name}: {result.error}\n\n"
            else:
                # CASO: success=True pero data es None o evalu√≥ a False
                self.logger.warning(f"‚ö†Ô∏è  WARNING: {tool_name} exitosa pero data es None o vac√≠o!")
                self.logger.warning(f"   result.data = {result.data}")
                message += f"‚ö†Ô∏è WARNING: {tool_name} se ejecut√≥ exitosamente pero no devolvi√≥ datos.\n\n"
        
        return message
    
    def _format_search_results(self, results: Dict[str, Any]) -> str:
        """Formatea resultados de b√∫squeda de forma resumida"""
        formatted = ""
        
        if 'fragments' in results and results['fragments']:
            total_fragments = len(results['fragments'])
            formatted += f"Se encontraron {total_fragments} fragmentos relevantes.\n\n"
            formatted += "**Informaci√≥n resumida de los fragmentos:**\n\n"
            
            # Agrupar por archivo
            files_dict = {}
            for fragment in results['fragments']:
                file_name = fragment.get('file_name', 'Desconocido')
                if file_name not in files_dict:
                    files_dict[file_name] = {
                        'count': 0,
                        'max_score': 0,
                        'content_preview': ''
                    }
                files_dict[file_name]['count'] += 1
                score = fragment.get('score', 0)
                if score > files_dict[file_name]['max_score']:
                    files_dict[file_name]['max_score'] = score
                    content = fragment.get('content', '')
                    files_dict[file_name]['content_preview'] = content[:500] if content else ''
            
            # Formatear resumen por archivo
            for i, (file_name, info) in enumerate(sorted(files_dict.items(), key=lambda x: x[1]['max_score'], reverse=True), 1):
                formatted += f"{i}. **{file_name}**\n"
                formatted += f"   - Fragmentos encontrados: {info['count']}\n"
                formatted += f"   - Relevancia m√°xima: {info['max_score']:.4f}\n"
                if info['content_preview']:
                    formatted += f"   - Vista previa: {info['content_preview'][:200]}...\n"
                formatted += "\n"
        else:
            formatted += "No se encontraron resultados.\n"
        
        return formatted
    
    def _format_file_content(self, results: Dict[str, Any]) -> str:
        """
        Formatea contenido de archivo COMPLETO para el LLM.
        
        Maneja dos modos:
        1. Modo completo: Env√≠a el contenido completo del archivo
        2. Modo progresivo: Env√≠a la estructura del documento para que el LLM solicite secciones espec√≠ficas
        """
        formatted = ""
        
        # Verificar si es modo progresivo
        access_mode = results.get('access_mode', 'full')
        
        if access_mode == 'progressive':
            # MODO PROGRESIVO: Archivo grande, enviar estructura
            import json
            
            file_path = results.get('file_path', 'archivo')
            content_length = results.get('content_length', 0)
            message = results.get('message', '')
            
            formatted += f"üìÑ **Archivo**: {file_path}\n"
            formatted += f"‚ö†Ô∏è  **Modo de acceso**: PROGRESIVO (archivo grande)\n"
            formatted += f"üìè **Tama√±o**: {content_length:,} caracteres\n\n"
            formatted += f"**Mensaje**: {message}\n\n"
            
            # Enviar estructura completa del documento
            if 'structure' in results:
                structure = results['structure']
                formatted += f"üìã **ESTRUCTURA DEL DOCUMENTO**:\n\n"
                formatted += f"```json\n{json.dumps(structure, indent=2, ensure_ascii=False)}\n```\n\n"
            
            # Secciones disponibles
            if 'available_sections' in results:
                formatted += f"üìë **Secciones disponibles**: {results['available_sections']}\n\n"
            
            # Rangos de chunks
            if 'chunk_ranges' in results:
                formatted += f"üìä **Rangos de chunks**: {results['chunk_ranges']}\n\n"
            
            # Recomendaci√≥n
            if 'recommendation' in results:
                formatted += f"üí° **Recomendaci√≥n**: {results['recommendation']}\n\n"
            
            formatted += "**INSTRUCCI√ìN**: Analiza la estructura y usa `tool_get_file_section` para obtener las secciones relevantes.\n"
            
        elif 'content' in results:
            # MODO COMPLETO: Archivo peque√±o, enviar contenido completo
            content = results['content']
            file_name = results.get('file_name', 'archivo')
            
            formatted += f"Archivo: {file_name}\n"
            formatted += f"Tama√±o del contenido: {len(content)} caracteres\n\n"
            formatted += f"Contenido completo del archivo:\n\n"
            formatted += f"```\n{content}\n```\n"
            
            if 'metadata' in results:
                formatted += f"\n**Metadata del archivo:**\n"
                for key, value in results['metadata'].items():
                    formatted += f"- {key}: {value}\n"
        else:
            # CASO DE ERROR: No hay ni modo progresivo ni contenido
            self.logger.error(f"‚ùå ERROR: _format_file_content recibi√≥ results sin 'access_mode' ni 'content'")
            self.logger.error(f"Results keys: {list(results.keys())}")
            self.logger.error(f"Results completo: {str(results)[:500]}")
            formatted += f"‚ö†Ô∏è ERROR: No se pudo formatear el contenido del archivo.\n"
            formatted += f"Estructura de resultados inesperada: {list(results.keys())}\n"
        
        return formatted
    
    def process_user_input_streaming(self, user_input: str) -> Optional[Dict[str, Any]]:
        """
        Procesa input del usuario con streaming habilitado e iteraci√≥n de herramientas
        
        Args:
            user_input: Input del usuario
            
        Returns:
            Diccionario con resultado del procesamiento o None si es comando especial
        """
        # Procesar comandos especiales
        if user_input.lower() == 'salir':
            self.stop()
            return None
        
        if user_input.lower() == 'historial':
            self._display_conversation_history()
            return None
        
        if user_input.lower() == 'estad√≠sticas':
            self._display_conversation_stats()
            return None
        
        if user_input.lower() == 'limpiar':
            self._clear_screen()
            return None
        
        if not user_input.strip():
            print(warning("‚ö†Ô∏è  Por favor, ingresa una consulta v√°lida.\n"))
            return None
        
        # Procesar consulta con streaming
        self.logger.debug(f"Procesando input con streaming: {user_input[:50]}...")
        
        try:
            import time
            start_time = time.time()
            
            # Agregar turno del usuario al historial
            self.llm_comm.conversation_manager.add_user_turn(
                session_id=self.session_id,
                message=user_input,
                tokens=len(user_input.split())
            )
            
            # Variables para el ciclo iterativo
            max_iterations = self.config_manager.get('agent.max_tool_iterations', 5)
            iteration = 0
            all_tool_results = []
            total_llm_time = 0
            total_tools_time = 0
            total_tokens_input = 0
            total_tokens_output = 0
            
            # Mensaje actual para enviar al LLM (inicialmente la pregunta del usuario)
            current_message = user_input
            
            # CICLO ITERATIVO: Continuar mientras haya herramientas que ejecutar
            while iteration < max_iterations:
                iteration += 1
                self.logger.info(f"üîÑ Iteraci√≥n {iteration}/{max_iterations}")
                
                # Crear componentes de streaming para esta iteraci√≥n con m√°quina de estados
                display = StreamingDisplay(enable_colors=True)
                state_machine = StreamingStateMachine(display)
                
                # Callback para procesar tokens con la m√°quina de estados
                def token_callback(token: str):
                    state_machine.feed_token(token)
                
                # Obtener historial conversacional (sin el √∫ltimo turno de usuario)
                conversation_context = self.llm_comm.conversation_manager.get_conversation_context(self.session_id)
                conversation_history = self.llm_comm._build_conversation_history(conversation_context)
                if conversation_history and conversation_history[-1]['role'] == 'user':
                    conversation_history = conversation_history[:-1]
                
                # Crear request
                llm_request = LLMRequest(
                    session_id=self.session_id,
                    system_prompt=self.llm_comm.system_prompt,
                    user_input=current_message,
                    conversation_history=conversation_history,
                    max_tokens=self.llm_comm.max_tokens,
                    temperature=self.llm_comm.temperature,
                    use_cache=True
                )
                
                # Enviar request con streaming
                llm_start = time.time()
                response = self.llm_comm.send_request_streaming(
                    llm_request=llm_request,
                    token_callback=token_callback
                )
                llm_time = (time.time() - llm_start) * 1000
                total_llm_time += llm_time
                
                # Finalizar m√°quina de estados (libera buffer pendiente)
                accumulated_text = state_machine.finalize()
                
                # Parsear el texto acumulado para extraer y ejecutar herramientas
                # Usar el m√©todo del tool_executor que parsea XML directamente
                tool_results = []
                try:
                    tool_calls = self.tool_executor.parse_tool_calls_from_xml(accumulated_text)
                    
                    if tool_calls:
                        self.logger.info(f"üîß Ejecutando {len(tool_calls)} herramientas...")
                        for tool_call in tool_calls:
                            try:
                                result = self.tool_executor.execute_tool(
                                    tool_call['tool_type'],
                                    tool_call['params']
                                )
                                tool_results.append(result)
                                
                                # Mostrar resultado de herramienta
                                if result.success:
                                    print(success(f"  ‚úÖ {result.tool_type.value}: {result.execution_time_ms:.0f}ms"))
                                else:
                                    print(error(f"  ‚ùå {result.tool_type.value}: {result.error}"))
                            except Exception as e:
                                self.logger.error(f"Error ejecutando herramienta: {e}")
                                print(error(f"  ‚ùå Error ejecutando herramienta: {str(e)}"))
                except Exception as e:
                    self.logger.error(f"Error parseando herramientas: {e}")
                
                # Actualizar historial con respuesta del asistente
                tools_used = [result.tool_type.value if hasattr(result.tool_type, 'value') else str(result.tool_type) 
                             for result in tool_results]
                self.llm_comm.conversation_manager.add_assistant_turn(
                    session_id=self.session_id,
                    response=response.content,
                    tools_used=tools_used,
                    tokens=response.usage.get('output_tokens', 0)
                )
                
                # Acumular m√©tricas
                total_tokens_input += response.usage.get('input_tokens', 0)
                total_tokens_output += response.usage.get('output_tokens', 0)
                
                if tool_results:
                    all_tool_results.extend(tool_results)
                    total_tools_time += sum(r.execution_time_ms for r in tool_results)
                    
                    self.logger.info(f"‚úÖ Iteraci√≥n {iteration}: {len(tool_results)} herramientas ejecutadas")
                    
                    # Formatear resultados para enviar al LLM
                    tool_results_message = self._format_tool_results_for_llm(
                        tool_results,
                        original_question=user_input
                    )
                    
                    # Mostrar info al usuario
                    print(info(f"\n  ‚ÑπÔ∏è  Resultados enviados al LLM ({len(tool_results_message)} caracteres)\n"))
                    
                    # Preparar mensaje para siguiente iteraci√≥n
                    current_message = tool_results_message
                    
                    # Agregar turno del usuario (resultados de herramientas) al historial
                    self.llm_comm.conversation_manager.add_user_turn(
                        session_id=self.session_id,
                        message=tool_results_message,
                        tokens=len(tool_results_message.split())
                    )
                else:
                    # No hay m√°s herramientas, salir del ciclo
                    self.logger.info(f"‚úÖ No hay m√°s herramientas en iteraci√≥n {iteration}. Finalizando.")
                    break
            
            # Verificar si se alcanz√≥ el m√°ximo de iteraciones
            if iteration >= max_iterations:
                self.logger.warning(f"‚ö†Ô∏è  Alcanzado m√°ximo de iteraciones ({max_iterations})")
            
            execution_time = (time.time() - start_time) * 1000
            
            # Registrar en log
            try:
                metrics = {
                    "total_time_ms": execution_time,
                    "llm_time_ms": total_llm_time,
                    "tools_time_ms": total_tools_time,
                    "tokens_input": total_tokens_input,
                    "tokens_output": total_tokens_output,
                    "cache_tokens_saved": 0,
                    "tools_executed": len(all_tool_results),
                    "tools_successful": sum(1 for r in all_tool_results if r.success),
                    "iterations": iteration
                }
                
                self.conversation_logger.log_conversation_turn(
                    session_id=self.session_id,
                    user_input=user_input,
                    llm_response=response.content,
                    metrics=metrics
                )
                
                self.logger.info(f"‚úÖ Turno registrado en logs/")
            
            except Exception as log_error:
                self.logger.warning(f"Error registrando turno en log: {log_error}")
            
            return {
                'response': response,
                'tool_results': all_tool_results,
                'execution_time_ms': execution_time,
                'iterations': iteration
            }
        
        except Exception as e:
            self.logger.error(f"Error procesando input con streaming: {str(e)}")
            
            # Registrar error en log
            try:
                self.conversation_logger.log_error(
                    session_id=self.session_id,
                    error_message=str(e),
                    error_type=type(e).__name__,
                    user_input=user_input,
                    traceback_info=traceback.format_exc()
                )
            except Exception as log_error:
                self.logger.warning(f"Error registrando error en log: {log_error}")
            
            print(error(f"‚ùå Error procesando tu consulta: {str(e)}\n"))
            return None
    
    def process_user_input(self, user_input: str) -> Optional[RequestResult]:
        """
        Procesa input del usuario (modo batch, sin streaming)
        
        Args:
            user_input: Input del usuario
            
        Returns:
            RequestResult o None si es comando especial
        """
        # Procesar comandos especiales
        if user_input.lower() == 'salir':
            self.stop()
            return None
        
        if user_input.lower() == 'historial':
            self._display_conversation_history()
            return None
        
        if user_input.lower() == 'estad√≠sticas':
            self._display_conversation_stats()
            return None
        
        if user_input.lower() == 'limpiar':
            self._clear_screen()
            return None
        
        if not user_input.strip():
            print(warning("‚ö†Ô∏è  Por favor, ingresa una consulta v√°lida.\n"))
            return None
        
        # Procesar consulta normal
        self.logger.debug(f"Procesando input del usuario: {user_input[:50]}...")
        
        try:
            result = self.request_handler.process_request(
                session_id=self.session_id,
                user_input=user_input
            )
            
            # Registrar petici√≥n y respuesta en log
            if result and result.state != RequestState.ERROR:
                try:
                    llm_response = result.formatted_response.filtered_content if result.formatted_response else "Sin respuesta"
                    metrics = {
                        "total_time_ms": result.metrics.total_time_ms,
                        "llm_time_ms": result.metrics.llm_time_ms,
                        "tools_time_ms": result.metrics.tools_time_ms,
                        "tokens_input": result.metrics.tokens_input,
                        "tokens_output": result.metrics.tokens_output,
                        "cache_tokens_saved": result.metrics.cache_tokens_saved,
                        "tools_executed": result.metrics.tools_executed,
                        "tools_successful": result.metrics.tools_successful
                    }
                    
                    self.conversation_logger.log_conversation_turn(
                        session_id=self.session_id,
                        user_input=user_input,
                        llm_response=llm_response,
                        metrics=metrics
                    )
                    
                    self.logger.info(f"‚úÖ Turno registrado en logs/")
                
                except Exception as log_error:
                    self.logger.warning(f"Error registrando turno en log: {log_error}")
            
            return result
        
        except Exception as e:
            self.logger.error(f"Error procesando input: {str(e)}")
            
            # Registrar error en log
            try:
                self.conversation_logger.log_error(
                    session_id=self.session_id,
                    error_message=str(e),
                    error_type=type(e).__name__,
                    user_input=user_input,
                    traceback_info=traceback.format_exc()
                )
            except Exception as log_error:
                self.logger.warning(f"Error registrando error en log: {log_error}")
            
            print(error(f"‚ùå Error procesando tu consulta: {str(e)}\n"))
            return None
    
    def display_result(self, result: RequestResult) -> None:
        """
        Muestra el resultado de un request (solo respuesta del LLM en pantalla)
        
        Args:
            result: Resultado del procesamiento
        """
        if result.state == RequestState.ERROR:
            print(error("‚ùå Error procesando la consulta. Por favor, intenta de nuevo.\n"))
            return
        
        # Mostrar respuesta formateada con color verde (respuesta del LLM)
        if result.formatted_response:
            print(llm_response(result.formatted_response.filtered_content))
            print()  # L√≠nea en blanco
    
    
    def _display_conversation_history(self) -> None:
        """Muestra el historial de conversaci√≥n"""
        history = self.request_handler.get_conversation_history(self.session_id)
        
        print("\n" + "="*64)
        print("üìú HISTORIAL DE CONVERSACI√ìN:")
        print("="*64 + "\n")
        
        if history.strip():
            print(history)
        else:
            print("No hay historial de conversaci√≥n a√∫n.\n")
        
        print("="*64 + "\n")
    
    def _display_conversation_stats(self) -> None:
        """Muestra estad√≠sticas de la conversaci√≥n"""
        stats = self.request_handler.get_conversation_stats(self.session_id)
        
        print("\n" + "="*64)
        print("üìä ESTAD√çSTICAS DE LA SESI√ìN:")
        print("="*64 + "\n")
        
        for key, value in stats.items():
            print(f"  ‚Ä¢ {key}: {value}")
        
        print("\n" + "="*64 + "\n")
    
    def _clear_screen(self) -> None:
        """Limpia la pantalla"""
        import os
        os.system('clear' if os.name == 'posix' else 'cls')
        self._display_welcome_message()
    
    def run_interactive(self) -> None:
        """Ejecuta la interfaz en modo interactivo"""
        self.start()
        
        try:
            while self.is_active:
                try:
                    # Capturar input del usuario
                    user_input = input(user_text("\nüë§ T√∫: ")).strip()
                    
                    if not user_input:
                        continue
                    
                    # Actualizar actividad de sesi√≥n
                    self.session_manager.update_session_activity()
                    
                    # Procesar input con o sin streaming
                    if self.enable_streaming:
                        result = self.process_user_input_streaming(user_input)
                    else:
                        result = self.process_user_input(user_input)
                        # Mostrar resultado si no es streaming
                        if result:
                            self.display_result(result)
                
                except KeyboardInterrupt:
                    print(warning("\n\n‚ö†Ô∏è  Sesi√≥n interrumpida por el usuario."))
                    self.stop()
                    break
                
                except EOFError:
                    print(warning("\n\n‚ö†Ô∏è  Fin de entrada detectado."))
                    self.stop()
                    break
        
        except Exception as e:
            self.logger.error(f"Error en modo interactivo: {str(e)}", exc_info=True)
            print(error(f"‚ùå Error: {str(e)}"))
    
    def run_batch(self, queries: list) -> None:
        """
        Ejecuta la interfaz en modo batch
        
        Args:
            queries: Lista de consultas a procesar
        """
        self.start()
        
        for i, query in enumerate(queries, 1):
            print(f"\n{'='*64}")
            print(f"üìù Consulta {i}/{len(queries)}: {query}")
            print('='*64)
            
            result = self.process_user_input(query)
            
            if result:
                self.display_result(result)
        
        self.stop()


def main():
    """Funci√≥n principal para testing"""
    import logging
    
    # Configurar logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # System prompt
    system_prompt = """Eres un asistente especializado en consultas sobre la base de conocimiento Darwin.
Tu objetivo es responder preguntas sobre aspectos funcionales y t√©cnicos del sistema.
Cuando necesites informaci√≥n, solicita el uso de herramientas de b√∫squeda mediante XML.
Siempre proporciona respuestas claras, precisas y bien estructuradas."""
    
    # Crear interfaz de chat
    chat = ChatInterface(system_prompt=system_prompt)
    
    # Ejecutar en modo interactivo
    chat.run_interactive()


if __name__ == "__main__":
    main()
