#!/usr/bin/env python3
"""
Script para extraer y guardar todas las imÃ¡genes de un PDF
Ãštil para anÃ¡lisis y debugging de detecciÃ³n de imÃ¡genes
"""

import os
import sys
import fitz  # PyMuPDF
import argparse
from pathlib import Path
from loguru import logger
import hashlib

def extract_images_from_pdf(pdf_path: str, output_dir: str, min_width: int = 0, min_height: int = 0, min_pixels: int = 0):
    """
    Extrae todas las imÃ¡genes de un PDF y las guarda en un directorio
    
    Args:
        pdf_path: Ruta al archivo PDF
        output_dir: Directorio donde guardar las imÃ¡genes
        min_width: Ancho mÃ­nimo para filtrar (0 = sin filtro)
        min_height: Alto mÃ­nimo para filtrar (0 = sin filtro)
        min_pixels: PÃ­xeles totales mÃ­nimos para filtrar (0 = sin filtro)
    """
    
    # Crear directorio de salida si no existe
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Abrir PDF
    logger.info(f"Abriendo PDF: {pdf_path}")
    pdf_document = fitz.open(pdf_path)
    
    # EstadÃ­sticas
    total_images = 0
    extracted_images = 0
    filtered_images = 0
    duplicate_images = 0
    seen_hashes = set()
    
    # Crear archivo de reporte
    report_path = output_path / "extraction_report.txt"
    report_lines = []
    report_lines.append(f"REPORTE DE EXTRACCIÃ“N DE IMÃGENES")
    report_lines.append(f"PDF: {pdf_path}")
    report_lines.append(f"Filtros aplicados: min_width={min_width}, min_height={min_height}, min_pixels={min_pixels}")
    report_lines.append("=" * 80)
    report_lines.append("")
    
    # Procesar cada pÃ¡gina
    for page_num in range(len(pdf_document)):
        page = pdf_document.load_page(page_num)
        image_list = page.get_images()
        
        logger.info(f"ðŸ“„ PÃ¡gina {page_num + 1}: {len(image_list)} imÃ¡genes detectadas")
        report_lines.append(f"\n--- PÃGINA {page_num + 1} ---")
        report_lines.append(f"ImÃ¡genes detectadas: {len(image_list)}")
        report_lines.append("")
        
        total_images += len(image_list)
        
        for img_index, img in enumerate(image_list):
            try:
                xref = img[0]
                
                # Obtener informaciÃ³n de la imagen
                try:
                    base_image = pdf_document.extract_image(xref)
                    img_ext = base_image.get('ext', 'png')
                    img_colorspace = base_image.get('colorspace', 'unknown')
                    img_bpc = base_image.get('bpc', 'unknown')
                except Exception as e:
                    img_ext = 'png'
                    img_colorspace = 'unknown'
                    img_bpc = 'unknown'
                
                # Crear Pixmap para obtener dimensiones
                pix = fitz.Pixmap(pdf_document, xref)
                width = pix.width
                height = pix.height
                total_pixels = width * height
                colorspace_pix = pix.colorspace.name if pix.colorspace else 'unknown'
                
                # InformaciÃ³n de la imagen
                img_info = f"Imagen {img_index + 1}: {width}x{height} ({total_pixels:,} px) - colorspace: {colorspace_pix}, ext: {img_ext}, bpc: {img_bpc}"
                
                # Aplicar filtros
                filtered = False
                filter_reasons = []
                
                if min_width > 0 and width < min_width:
                    filtered = True
                    filter_reasons.append(f"width {width} < {min_width}")
                
                if min_height > 0 and height < min_height:
                    filtered = True
                    filter_reasons.append(f"height {height} < {min_height}")
                
                if min_pixels > 0 and total_pixels < min_pixels:
                    filtered = True
                    filter_reasons.append(f"pixels {total_pixels} < {min_pixels}")
                
                if filtered:
                    logger.debug(f"ðŸ” {img_info} - FILTRADA: {', '.join(filter_reasons)}")
                    report_lines.append(f"  {img_info} - FILTRADA: {', '.join(filter_reasons)}")
                    filtered_images += 1
                    pix = None
                    continue
                
                # Convertir a PNG si es posible
                if pix.n - pix.alpha < 4:  # GRAY or RGB
                    img_data = pix.tobytes("png")
                    
                    # Calcular hash para detectar duplicados
                    img_hash = hashlib.md5(img_data).hexdigest()
                    
                    # Verificar si es duplicado
                    if img_hash in seen_hashes:
                        logger.debug(f"ðŸ”„ {img_info} - DUPLICADA (hash: {img_hash[:8]})")
                        report_lines.append(f"  {img_info} - DUPLICADA (hash: {img_hash[:8]})")
                        duplicate_images += 1
                        pix = None
                        continue
                    
                    seen_hashes.add(img_hash)
                    
                    # Guardar imagen
                    filename = f"page{page_num + 1:03d}_img{img_index + 1:03d}_{width}x{height}_{img_hash[:8]}.png"
                    filepath = output_path / filename
                    
                    with open(filepath, "wb") as img_file:
                        img_file.write(img_data)
                    
                    extracted_images += 1
                    logger.info(f"âœ… {img_info} - GUARDADA: {filename}")
                    report_lines.append(f"  {img_info} - GUARDADA: {filename}")
                else:
                    logger.debug(f"âš ï¸  {img_info} - COLORSPACE NO SOPORTADO (n={pix.n}, alpha={pix.alpha})")
                    report_lines.append(f"  {img_info} - COLORSPACE NO SOPORTADO")
                
                pix = None
                
            except Exception as e:
                logger.error(f"Error extrayendo imagen {img_index + 1} de pÃ¡gina {page_num + 1}: {e}")
                report_lines.append(f"  Imagen {img_index + 1}: ERROR - {e}")
    
    # Resumen final
    report_lines.append("")
    report_lines.append("=" * 80)
    report_lines.append("RESUMEN")
    report_lines.append("=" * 80)
    report_lines.append(f"Total de imÃ¡genes detectadas: {total_images}")
    report_lines.append(f"ImÃ¡genes extraÃ­das y guardadas: {extracted_images}")
    report_lines.append(f"ImÃ¡genes filtradas (no cumplen requisitos): {filtered_images}")
    report_lines.append(f"ImÃ¡genes duplicadas (omitidas): {duplicate_images}")
    report_lines.append("")
    
    # Guardar reporte
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    logger.info("")
    logger.info("=" * 80)
    logger.info("ðŸ“Š RESUMEN DE EXTRACCIÃ“N")
    logger.info("=" * 80)
    logger.info(f"Total de imÃ¡genes detectadas: {total_images}")
    logger.info(f"ImÃ¡genes extraÃ­das y guardadas: {extracted_images}")
    logger.info(f"ImÃ¡genes filtradas: {filtered_images}")
    logger.info(f"ImÃ¡genes duplicadas: {duplicate_images}")
    logger.info(f"Directorio de salida: {output_path.absolute()}")
    logger.info(f"Reporte guardado en: {report_path.absolute()}")
    logger.info("=" * 80)
    
    pdf_document.close()
    
    return {
        'total': total_images,
        'extracted': extracted_images,
        'filtered': filtered_images,
        'duplicates': duplicate_images,
        'output_dir': str(output_path.absolute())
    }


def main():
    parser = argparse.ArgumentParser(
        description='Extrae y guarda todas las imÃ¡genes de un PDF',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:

  # Extraer TODAS las imÃ¡genes (sin filtros)
  python3 src/utils/extract_pdf_images.py documento.pdf -o imagenes_extraidas

  # Extraer solo imÃ¡genes grandes (con filtros de producciÃ³n)
  python3 src/utils/extract_pdf_images.py documento.pdf -o imagenes_grandes \\
    --min-width 500 --min-height 500 --min-pixels 250000

  # Extraer imÃ¡genes medianas
  python3 src/utils/extract_pdf_images.py documento.pdf -o imagenes_medianas \\
    --min-width 100 --min-height 100

  # Desde S3 (primero descargar el archivo)
  aws s3 cp s3://bucket/documento.pdf /tmp/documento.pdf
  python3 src/utils/extract_pdf_images.py /tmp/documento.pdf -o imagenes_s3
        """
    )
    
    parser.add_argument('pdf_path', help='Ruta al archivo PDF')
    parser.add_argument('-o', '--output', required=True, help='Directorio de salida para las imÃ¡genes')
    parser.add_argument('--min-width', type=int, default=0, help='Ancho mÃ­nimo en pÃ­xeles (0 = sin filtro)')
    parser.add_argument('--min-height', type=int, default=0, help='Alto mÃ­nimo en pÃ­xeles (0 = sin filtro)')
    parser.add_argument('--min-pixels', type=int, default=0, help='PÃ­xeles totales mÃ­nimos (0 = sin filtro)')
    
    args = parser.parse_args()
    
    # Verificar que el PDF existe
    if not os.path.exists(args.pdf_path):
        logger.error(f"El archivo PDF no existe: {args.pdf_path}")
        sys.exit(1)
    
    # Extraer imÃ¡genes
    try:
        result = extract_images_from_pdf(
            pdf_path=args.pdf_path,
            output_dir=args.output,
            min_width=args.min_width,
            min_height=args.min_height,
            min_pixels=args.min_pixels
        )
        
        logger.success(f"âœ… ExtracciÃ³n completada: {result['extracted']} imÃ¡genes guardadas en {result['output_dir']}")
        
    except Exception as e:
        logger.error(f"Error durante la extracciÃ³n: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
