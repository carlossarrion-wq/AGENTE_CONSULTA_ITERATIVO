"""
Update System Prompt with Web Crawler Tool

Script para actualizar din√°micamente los system prompts con la documentaci√≥n
de la herramienta web crawler, bas√°ndose en la configuraci√≥n.
"""

import yaml
from pathlib import Path
from typing import Dict, Any


def load_web_crawler_config(config_path: str = "config/web_crawler_config.yaml") -> Dict[str, Any]:
    """Carga la configuraci√≥n del web crawler"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def generate_web_crawler_documentation(app_name: str, config: Dict[str, Any]) -> str:
    """
    Genera la documentaci√≥n de la herramienta web crawler para un app espec√≠fica
    
    Args:
        app_name: Nombre de la aplicaci√≥n (mulesoft, darwin, sap)
        config: Configuraci√≥n completa del web crawler
        
    Returns:
        String con la documentaci√≥n formateada
    """
    app_config = config.get(app_name.lower(), {})
    
    # Verificar si est√° habilitada
    if not app_config.get('enabled', False):
        return ""
    
    global_config = config.get('global', {})
    
    # Obtener listas de configuraci√≥n
    allowed_domains = app_config.get('allowed_domains', [])
    required_keywords = app_config.get('required_keywords', [])
    forbidden_keywords = app_config.get('forbidden_keywords', [])
    
    # Formatear dominios permitidos
    domains_list = "\n".join([f"     - {domain}" for domain in allowed_domains])
    
    # Formatear keywords requeridas
    required_list = ", ".join([f'"{kw}"' for kw in required_keywords])
    
    # Formatear keywords prohibidas
    forbidden_list = ", ".join([f'"{kw}"' for kw in forbidden_keywords]) if forbidden_keywords else "ninguna"
    
    documentation = f"""
## 5. tool_web_crawler - B√∫squeda en Internet

**Prop√≥sito**: Buscar informaci√≥n actualizada en internet cuando la base de conocimiento interna no es suficiente.

**Cu√°ndo usar**:
- Cuando necesites informaci√≥n sobre patches, actualizaciones o versiones recientes
- Para consultar documentaci√≥n oficial actualizada
- Cuando la informaci√≥n en la base de conocimiento est√© desactualizada
- Para verificar problemas conocidos o release notes recientes

**Restricciones de Seguridad**:
- **Dominios Permitidos**: Solo se pueden buscar en los siguientes dominios autorizados:
{domains_list}

- **Keywords Requeridas**: Toda b√∫squeda DEBE incluir al menos una de estas palabras: {required_list}

- **Keywords Prohibidas**: NO se permiten b√∫squedas con: {forbidden_list}

- **Rate Limiting**: M√≠nimo {app_config.get('rate_limit', {}).get('min_delay_seconds', 3)} segundos entre b√∫squedas

- **L√≠mites**: M√°ximo {global_config.get('max_results_per_search', 3)} resultados por b√∫squeda, {global_config.get('max_content_length', 2000)} caracteres por resultado

**Formato XML**:
```xml
<tool_web_crawler>
<query>tu consulta de b√∫squeda aqu√≠</query>
<app_name>{app_name.lower()}</app_name>
</tool_web_crawler>
```

**Par√°metros**:
- `query` (requerido): Consulta de b√∫squeda. DEBE incluir keywords requeridas y estar relacionada con {app_name}
- `app_name` (opcional): Nombre de la aplicaci√≥n. Default: "{app_name.lower()}"

**Ejemplo de Uso**:
```xml
<tool_web_crawler>
<query>{app_name.lower()} runtime 4.5.0 security patch latest</query>
<app_name>{app_name.lower()}</app_name>
</tool_web_crawler>
```

**Respuesta**:
La herramienta devuelve un JSON con:
- `query`: La consulta realizada
- `results_count`: N√∫mero de resultados encontrados
- `sources`: Array de fuentes con:
  - `number`: N√∫mero del resultado
  - `url`: URL de la fuente (siempre de dominios permitidos)
  - `content`: Contenido extra√≠do (m√°ximo {global_config.get('max_content_length', 2000)} caracteres)
  - `extraction_method`: M√©todo usado (trafilatura, beautifulsoup, o cache)

**Ejemplo de Respuesta**:
```json
{{
  "query": "{app_name.lower()} api security patches",
  "results_count": 3,
  "sources": [
    {{
      "number": 1,
      "url": "https://docs.{app_name.lower()}.com/release-notes/...",
      "content": "Contenido extra√≠do de la p√°gina...",
      "extraction_method": "trafilatura"
    }}
  ]
}}
```

**Notas Importantes**:
1. La herramienta valida autom√°ticamente que las URLs est√©n en dominios permitidos
2. Las b√∫squedas se cachean por {global_config.get('cache_ttl_hours', 24)} horas para reducir requests
3. Si una b√∫squeda no cumple con las restricciones, ser√° rechazada autom√°ticamente
4. Usa esta herramienta solo cuando la informaci√≥n interna no sea suficiente o est√© desactualizada
"""
    
    return documentation


def update_system_prompt(app_name: str, prompt_path: str, web_crawler_config: Dict[str, Any]) -> bool:
    """
    Actualiza un system prompt con la documentaci√≥n del web crawler
    
    Args:
        app_name: Nombre de la aplicaci√≥n
        prompt_path: Ruta al archivo de system prompt
        web_crawler_config: Configuraci√≥n del web crawler
        
    Returns:
        True si se actualiz√≥ correctamente
    """
    try:
        # Leer el prompt actual
        with open(prompt_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Generar documentaci√≥n
        web_crawler_doc = generate_web_crawler_documentation(app_name, web_crawler_config)
        
        # Si no est√° habilitada, remover cualquier documentaci√≥n existente
        if not web_crawler_doc:
            # Buscar y remover secci√≥n existente
            start_marker = "## 5. tool_web_crawler"
            if start_marker in content:
                # Encontrar el inicio de la siguiente secci√≥n o el final
                start_idx = content.find(start_marker)
                # Buscar la siguiente secci√≥n que empiece con ##
                next_section_idx = content.find("\n## ", start_idx + 1)
                
                if next_section_idx != -1:
                    content = content[:start_idx] + content[next_section_idx:]
                else:
                    content = content[:start_idx]
                
                # Guardar
                with open(prompt_path, 'w', encoding='utf-8') as f:
                    f.write(content.rstrip() + "\n")
                
                print(f"‚úÖ Removida documentaci√≥n de web_crawler de {prompt_path} (herramienta deshabilitada)")
            else:
                print(f"‚ÑπÔ∏è  {prompt_path} no contiene documentaci√≥n de web_crawler")
            
            return True
        
        # Buscar d√≥nde insertar la documentaci√≥n
        # Buscar la secci√≥n de herramientas
        tools_section_marker = "# HERRAMIENTAS DISPONIBLES"
        
        if tools_section_marker not in content:
            print(f"‚ö†Ô∏è  No se encontr√≥ la secci√≥n de herramientas en {prompt_path}")
            return False
        
        # Verificar si ya existe documentaci√≥n del web crawler
        web_crawler_marker = "## 5. tool_web_crawler"
        
        if web_crawler_marker in content:
            # Reemplazar documentaci√≥n existente
            start_idx = content.find(web_crawler_marker)
            # Buscar la siguiente secci√≥n o el final
            next_section_idx = content.find("\n## ", start_idx + 1)
            
            if next_section_idx != -1:
                content = content[:start_idx] + web_crawler_doc.strip() + "\n\n" + content[next_section_idx:]
            else:
                content = content[:start_idx] + web_crawler_doc.strip() + "\n"
            
            print(f"‚úÖ Actualizada documentaci√≥n de web_crawler en {prompt_path}")
        else:
            # Insertar nueva documentaci√≥n despu√©s de la herramienta 4
            tool4_marker = "## 4. tool_get_file_content"
            
            if tool4_marker in content:
                # Encontrar el final de la secci√≥n de tool_get_file_content
                start_idx = content.find(tool4_marker)
                # Buscar la siguiente secci√≥n que empiece con ##
                next_section_idx = content.find("\n## ", start_idx + 1)
                
                if next_section_idx != -1:
                    # Insertar antes de la siguiente secci√≥n
                    content = content[:next_section_idx] + "\n" + web_crawler_doc.strip() + "\n" + content[next_section_idx:]
                else:
                    # Insertar al final
                    content = content.rstrip() + "\n\n" + web_crawler_doc.strip() + "\n"
                
                print(f"‚úÖ A√±adida documentaci√≥n de web_crawler a {prompt_path}")
            else:
                print(f"‚ö†Ô∏è  No se encontr√≥ la secci√≥n de tool_get_file_content en {prompt_path}")
                return False
        
        # Guardar el archivo actualizado
        with open(prompt_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error actualizando {prompt_path}: {e}")
        return False


def main():
    """Funci√≥n principal"""
    print("üîß Actualizando system prompts con documentaci√≥n de web crawler...\n")
    
    # Cargar configuraci√≥n del web crawler
    web_crawler_config = load_web_crawler_config()
    
    # Definir los prompts a actualizar
    prompts = {
        'mulesoft': 'config/system_prompt_mulesoft.md',
        'darwin': 'config/system_prompt_darwin.md',
        'sap': 'config/system_prompt_sap.md'
    }
    
    success_count = 0
    
    for app_name, prompt_path in prompts.items():
        print(f"\nüìù Procesando {app_name}...")
        
        # Verificar si el archivo existe
        if not Path(prompt_path).exists():
            print(f"‚ö†Ô∏è  Archivo no encontrado: {prompt_path}")
            continue
        
        # Verificar si est√° habilitado
        app_config = web_crawler_config.get(app_name.lower(), {})
        enabled = app_config.get('enabled', False)
        
        if enabled:
            print(f"   ‚úì Web crawler habilitado para {app_name}")
        else:
            print(f"   ‚úó Web crawler deshabilitado para {app_name}")
        
        # Actualizar prompt
        if update_system_prompt(app_name, prompt_path, web_crawler_config):
            success_count += 1
    
    print(f"\n{'='*60}")
    print(f"‚úÖ Actualizados {success_count}/{len(prompts)} system prompts")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
