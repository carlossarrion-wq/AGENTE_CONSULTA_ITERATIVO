# Configuración para las herramientas DeltaSmile
# Esta configuración usa conexión real a OpenSearch y AWS

# Configuración de OpenSearch
# DESARROLLO (localhost con túnel SSH):
opensearch:
  host: "localhost"
  port: 9201
  use_ssl: true
  verify_certs: false
  region: "eu-west-1"
  timeout: 30
  max_retries: 3
  index_name: "rag-documents-deltasmile"
  scroll_timeout: "2m"
  mock_mode: false

# PRODUCCIÓN EC2 (descomentar para usar en EC2):
# opensearch:
#   host: "vpc-rag-opensearch-clean-qodnaopeuroal2f6intbz7i5xy.eu-west-1.es.amazonaws.com"
#   port: 443
#   use_ssl: true
#   verify_certs: true
#   region: "eu-west-1"
#   timeout: 30
#   max_retries: 3
#   index_name: "rag-documents-deltasmile"
#   scroll_timeout: "2m"
#   mock_mode: false

# Configuración de AWS Bedrock
bedrock:
  region_name: "eu-west-1"
  model_id: "amazon.titan-embed-image-v1"
  embedding_dimensions: 1024
  mock_mode: false

# Configuración de AWS S3
s3:
  bucket_arn: "arn:aws:s3:::rag-system-deltasmile-eu-west-1"
  bucket_name: "rag-system-deltasmile-eu-west-1"
  region_name: "eu-west-1"
  prefix: "applications/deltasmile/"
  mock_mode: false

# Configuración por defecto para las herramientas
defaults:
  semantic_search:
    top_k: 10
    min_score: 0.5
    max_results: 1000
  
  lexical_search:
    top_k: 10
    operator: "OR"
    fuzzy: false
    fields: ["content"]
    fragment_size: 150
    number_of_fragments: 3
  
  regex_search:
    case_sensitive: true
    max_matches_per_file: 50
    context_lines: 2
    max_documents: 1000
  
  get_file_content:
    include_metadata: false
    batch_size: 100
    min_overlap: 50
    similarity_threshold: 0.85

# Configuración de logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/search_tools.log"

# Configuración de cache
cache:
  enabled: true
  max_size_mb: 100
  ttl_seconds: 3600

# Configuración del agente
agent:
  max_tool_iterations: 10
  enable_tool_execution: true
  system_prompt_file: "config/system_prompt_deltasmile.md"

# Configuración del LLM
llm:
  model_id: "eu.anthropic.claude-haiku-4-5-20251001-v1:0"
  max_tokens: 4000
  temperature: 0.1
  max_retries: 3
  retry_delay_seconds: 3

# Configuración de conversación
conversation:
  max_history_turns: 15
  context_window_tokens: 180000
  system_prompt_caching: true
  tool_results_caching: true
  enable_sliding_window: true
  min_turns_to_keep: 3

# Configuración de prompt caching
prompt_caching:
  enabled: true
  cache_ttl_seconds: 3600
  max_cache_size_mb: 500

# Límites de rendimiento
limits:
  max_content_length: 10485760  # 10MB
  max_chunks_per_file: 1000
  max_search_time_seconds: 30
