# Sistema de Aprendizaje mediante Feedback - DiseÃ±o

## Pregunta del Usuario
Â¿Es posible que el agente aprenda recibiendo feedback sobre las respuestas?

## Respuesta: SÃ­, es totalmente posible

Existen varias estrategias para implementar aprendizaje mediante feedback en tu agente IA. A continuaciÃ³n, presento diferentes aproximaciones ordenadas de menor a mayor complejidad.

---

## ğŸ¯ Estrategias de ImplementaciÃ³n

### 1. **Feedback ExplÃ­cito con Almacenamiento (MÃ¡s Simple)**

#### Concepto
DespuÃ©s de cada respuesta, el usuario puede calificar la respuesta (ğŸ‘/ğŸ‘) y opcionalmente proporcionar comentarios.

#### ImplementaciÃ³n
```python
# DespuÃ©s de mostrar la respuesta del LLM
feedback = input("Â¿Fue Ãºtil esta respuesta? (s/n/comentario): ")

if feedback.lower() in ['s', 'si', 'sÃ­', 'yes']:
    rating = 'positive'
elif feedback.lower() in ['n', 'no']:
    rating = 'negative'
else:
    rating = 'neutral'
    comment = feedback

# Guardar en logs
feedback_data = {
    "session_id": session_id,
    "query": user_input,
    "response": llm_response,
    "rating": rating,
    "comment": comment,
    "timestamp": datetime.now().isoformat()
}
```

#### Uso del Feedback
1. **AnÃ¡lisis manual**: Revisar respuestas mal calificadas para mejorar system prompts
2. **Ejemplos few-shot**: Usar respuestas bien calificadas como ejemplos en el prompt
3. **DetecciÃ³n de patrones**: Identificar tipos de preguntas que generan feedback negativo

#### Ventajas
- âœ… FÃ¡cil de implementar
- âœ… No requiere reentrenamiento del modelo
- âœ… Mejora inmediata mediante ajuste de prompts
- âœ… Bajo costo

#### Desventajas
- âŒ No modifica el modelo base
- âŒ Requiere intervenciÃ³n manual para mejoras

---

### 2. **Feedback con Few-Shot Learning DinÃ¡mico (Recomendado)**

#### Concepto
Usar el feedback para construir dinÃ¡micamente ejemplos few-shot que se incluyen en el system prompt.

#### Arquitectura
```
User Query â†’ Check Feedback DB â†’ Add Similar Good Examples â†’ LLM â†’ Response
                                        â†“
                                  User Feedback
                                        â†“
                                  Update Feedback DB
```

#### ImplementaciÃ³n

**Base de Datos de Feedback**
```python
class FeedbackDatabase:
    def __init__(self):
        self.feedback_file = "logs/feedback/feedback_db.json"
        self.positive_examples = []
        self.negative_examples = []
    
    def add_feedback(self, query, response, rating, comment=None):
        """Almacena feedback con embeddings para bÃºsqueda semÃ¡ntica"""
        feedback_entry = {
            "query": query,
            "response": response,
            "rating": rating,
            "comment": comment,
            "timestamp": datetime.now().isoformat(),
            "embedding": self._get_embedding(query)  # Para bÃºsqueda semÃ¡ntica
        }
        
        if rating == 'positive':
            self.positive_examples.append(feedback_entry)
        else:
            self.negative_examples.append(feedback_entry)
        
        self._save_to_disk()
    
    def get_similar_positive_examples(self, query, top_k=3):
        """Obtiene ejemplos positivos similares a la query actual"""
        query_embedding = self._get_embedding(query)
        
        # Calcular similitud coseno con ejemplos positivos
        similarities = []
        for example in self.positive_examples:
            similarity = cosine_similarity(query_embedding, example['embedding'])
            similarities.append((similarity, example))
        
        # Retornar top_k mÃ¡s similares
        similarities.sort(reverse=True, key=lambda x: x[0])
        return [ex for _, ex in similarities[:top_k]]
```

**IntegraciÃ³n en el Prompt**
```python
def build_enhanced_prompt(self, user_query, feedback_db):
    """Construye prompt con ejemplos de feedback positivo"""
    
    # Obtener ejemplos similares bien calificados
    similar_examples = feedback_db.get_similar_positive_examples(user_query, top_k=3)
    
    # Construir secciÃ³n de ejemplos
    examples_section = "\n\n# EJEMPLOS DE RESPUESTAS EXITOSAS\n\n"
    for i, example in enumerate(similar_examples, 1):
        examples_section += f"""
Ejemplo {i}:
Usuario: {example['query']}
Respuesta exitosa: {example['response']}
---
"""
    
    # Agregar al system prompt
    enhanced_prompt = self.base_system_prompt + examples_section
    return enhanced_prompt
```

#### Ventajas
- âœ… Mejora continua automÃ¡tica
- âœ… Aprende de ejemplos reales
- âœ… No requiere reentrenamiento
- âœ… Contextualmente relevante (usa similitud semÃ¡ntica)
- âœ… ImplementaciÃ³n moderada

#### Desventajas
- âŒ Requiere gestiÃ³n de base de datos de feedback
- âŒ Aumenta tokens en el prompt (costo)

---

### 3. **Reinforcement Learning from Human Feedback (RLHF) - Avanzado**

#### Concepto
Entrenar un modelo de recompensa basado en feedback humano y usar RL para optimizar el modelo.

#### Proceso
1. **RecolecciÃ³n de Feedback**: Usuarios califican respuestas
2. **Entrenamiento de Reward Model**: Modelo que predice quÃ© respuestas serÃ¡n bien calificadas
3. **OptimizaciÃ³n con PPO**: Ajustar el modelo usando Proximal Policy Optimization

#### ImplementaciÃ³n (Conceptual)
```python
# 1. Recolectar pares (query, response, rating)
feedback_dataset = collect_feedback_over_time()

# 2. Entrenar reward model
reward_model = train_reward_model(feedback_dataset)

# 3. Usar RL para optimizar
optimized_model = ppo_training(
    base_model=llm,
    reward_model=reward_model,
    training_data=queries
)
```

#### Ventajas
- âœ… Verdadero aprendizaje del modelo
- âœ… Mejora continua y automÃ¡tica
- âœ… AdaptaciÃ³n especÃ­fica a tu dominio

#### Desventajas
- âŒ Muy complejo de implementar
- âŒ Requiere infraestructura de ML
- âŒ Alto costo computacional
- âŒ Necesita gran cantidad de feedback
- âŒ No aplicable a modelos propietarios (Bedrock)

---

### 4. **Fine-Tuning con Feedback (Intermedio-Avanzado)**

#### Concepto
Usar feedback acumulado para crear dataset de fine-tuning y ajustar el modelo.

#### Proceso
```python
# 1. Acumular feedback positivo
positive_examples = [
    {"query": "...", "response": "...", "rating": "positive"},
    # ... mÃ¡s ejemplos
]

# 2. Convertir a formato de fine-tuning
training_data = []
for example in positive_examples:
    training_data.append({
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": example['query']},
            {"role": "assistant", "content": example['response']}
        ]
    })

# 3. Fine-tune (si el modelo lo permite)
fine_tuned_model = bedrock.create_fine_tuning_job(
    base_model="anthropic.claude-3-sonnet",
    training_data=training_data
)
```

#### Ventajas
- âœ… Modelo adaptado a tu dominio
- âœ… Mejora permanente
- âœ… Reduce necesidad de prompts largos

#### Desventajas
- âŒ No todos los modelos permiten fine-tuning
- âŒ Bedrock tiene limitaciones de fine-tuning
- âŒ Costo de entrenamiento
- âŒ Requiere cantidad significativa de datos

---

## ğŸ¯ RecomendaciÃ³n para tu Caso

### **Estrategia HÃ­brida: Feedback + Few-Shot DinÃ¡mico**

Te recomiendo implementar la **Estrategia 2** (Few-Shot Learning DinÃ¡mico) porque:

1. **Es prÃ¡ctica y efectiva**: Mejora inmediata sin reentrenar
2. **Bajo costo**: Solo aumenta tokens en el prompt
3. **Funciona con Bedrock**: No requiere acceso al modelo
4. **Escalable**: Puedes empezar simple y evolucionar
5. **Medible**: Puedes ver mejoras en mÃ©tricas de satisfacciÃ³n

### Arquitectura Propuesta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Query                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Feedback Database                               â”‚
â”‚  â€¢ Buscar ejemplos similares positivos                       â”‚
â”‚  â€¢ Extraer top-3 respuestas exitosas                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Enhanced System Prompt                             â”‚
â”‚  â€¢ System prompt base                                        â”‚
â”‚  â€¢ + Ejemplos de feedback positivo                           â”‚
â”‚  â€¢ + Contexto de quÃ© evitar (feedback negativo)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LLM (Bedrock)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Response                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              User Feedback                                   â”‚
â”‚  â€¢ Rating: ğŸ‘ / ğŸ‘                                           â”‚
â”‚  â€¢ Optional comment                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Update Feedback Database                             â”‚
â”‚  â€¢ Store with embeddings                                     â”‚
â”‚  â€¢ Update statistics                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Plan de ImplementaciÃ³n

### Fase 1: Infraestructura BÃ¡sica (1-2 dÃ­as)
- [ ] Crear mÃ³dulo `feedback_manager.py`
- [ ] Implementar captura de feedback en chat_interface
- [ ] Crear estructura de almacenamiento en logs/feedback/
- [ ] AÃ±adir comandos: `feedback`, `ver_feedback`

### Fase 2: Base de Datos de Feedback (2-3 dÃ­as)
- [ ] Implementar FeedbackDatabase class
- [ ] Integrar con OpenSearch para embeddings
- [ ] Crear bÃºsqueda semÃ¡ntica de ejemplos similares
- [ ] Implementar persistencia en JSON/SQLite

### Fase 3: Few-Shot DinÃ¡mico (2-3 dÃ­as)
- [ ] Modificar prompt builder para incluir ejemplos
- [ ] Implementar selecciÃ³n de top-k ejemplos similares
- [ ] AÃ±adir lÃ³gica de "quÃ© evitar" con feedback negativo
- [ ] Optimizar tamaÃ±o de prompt (limitar ejemplos)

### Fase 4: Analytics y Mejora Continua (1-2 dÃ­as)
- [ ] Dashboard de mÃ©tricas de feedback
- [ ] AnÃ¡lisis de patrones en feedback negativo
- [ ] Alertas para respuestas consistentemente mal calificadas
- [ ] Exportar datos para anÃ¡lisis manual

### Fase 5: Optimizaciones (Opcional)
- [ ] Cache de embeddings para performance
- [ ] Clustering de queries similares
- [ ] A/B testing de diferentes estrategias de few-shot
- [ ] IntegraciÃ³n con sistema de mÃ©tricas existente

---

## ğŸ’¡ Ejemplo de Uso

```bash
# Usuario inicia sesiÃ³n
python3 src/agent/main.py --app darwin --username darwin_001

# Usuario hace pregunta
ğŸ‘¤ TÃº: Â¿CÃ³mo crear un contrato en Darwin?

# Agente busca ejemplos similares bien calificados
[Sistema busca en feedback DB...]
[Encuentra 3 ejemplos similares con rating positivo]
[Construye prompt con estos ejemplos]

# Agente responde
ğŸ¤– Asistente: Para crear un contrato en Darwin...

# Usuario da feedback
Â¿Fue Ãºtil esta respuesta? (ğŸ‘/ğŸ‘/comentario): ğŸ‘

# Sistema almacena feedback
âœ… Feedback registrado. Â¡Gracias!

# PrÃ³xima vez que alguien pregunte algo similar
# El sistema usarÃ¡ esta respuesta como ejemplo
```

---

## ğŸ“Š MÃ©tricas de Ã‰xito

1. **Tasa de Feedback Positivo**: % de respuestas con ğŸ‘
2. **Mejora Temporal**: Comparar feedback antes/despuÃ©s de implementaciÃ³n
3. **ReducciÃ³n de Iteraciones**: Menos uso de herramientas por query
4. **SatisfacciÃ³n del Usuario**: Encuestas periÃ³dicas
5. **Cobertura de Ejemplos**: % de queries con ejemplos similares disponibles

---

## ğŸš€ PrÃ³ximos Pasos

Â¿Te gustarÃ­a que implemente esta soluciÃ³n? Puedo:

1. **Implementar Fase 1**: Sistema bÃ¡sico de captura de feedback
2. **Implementar Fase 2**: Base de datos con bÃºsqueda semÃ¡ntica
3. **Implementar Fase 3**: Few-shot learning dinÃ¡mico
4. **Crear prototipo completo**: Todas las fases integradas

Â¿Por cuÃ¡l fase te gustarÃ­a empezar?
