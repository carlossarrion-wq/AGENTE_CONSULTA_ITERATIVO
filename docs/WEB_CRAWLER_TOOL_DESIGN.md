# Herramienta de Web Crawler - Dise√±o e Implementaci√≥n

## Objetivo

Desarrollar una herramienta `tool_web_crawler` que permita al agente buscar informaci√≥n en internet cuando la base de conocimiento interna no sea suficiente.

## Casos de Uso

1. **Parches de MuleSoft**: Buscar parches recientes para problemas identificados
2. **Documentaci√≥n actualizada**: Consultar documentaci√≥n oficial m√°s reciente
3. **Errores conocidos**: Buscar soluciones a errores espec√≠ficos
4. **Versiones y releases**: Informaci√≥n sobre nuevas versiones
5. **Best practices**: Pr√°cticas recomendadas actualizadas

---

## üìö Librer√≠as Python Disponibles (Open Source)

### 1. **Beautiful Soup 4** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Mejor para**: Parsing de HTML/XML

```python
from bs4 import BeautifulSoup
import requests

# Ejemplo
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')
text = soup.get_text()
```

**Ventajas:**
- ‚úÖ Muy popular y bien documentada
- ‚úÖ F√°cil de usar
- ‚úÖ Excelente para extraer contenido estructurado
- ‚úÖ Maneja HTML mal formado

**Desventajas:**
- ‚ùå No ejecuta JavaScript
- ‚ùå Requiere requests para descargar p√°ginas

**Instalaci√≥n:**
```bash
pip3 install beautifulsoup4 requests
```

---

### 2. **Scrapy** ‚≠ê‚≠ê‚≠ê‚≠ê
**Mejor para**: Web scraping a gran escala

```python
import scrapy

class MulesoftSpider(scrapy.Spider):
    name = 'mulesoft'
    start_urls = ['https://docs.mulesoft.com']
    
    def parse(self, response):
        for article in response.css('article'):
            yield {
                'title': article.css('h1::text').get(),
                'content': article.css('p::text').getall()
            }
```

**Ventajas:**
- ‚úÖ Framework completo de scraping
- ‚úÖ Manejo autom√°tico de robots.txt
- ‚úÖ Soporte para crawling recursivo
- ‚úÖ Muy eficiente

**Desventajas:**
- ‚ùå M√°s complejo de configurar
- ‚ùå Overkill para b√∫squedas simples
- ‚ùå No ejecuta JavaScript

**Instalaci√≥n:**
```bash
pip3 install scrapy
```

---

### 3. **Selenium** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Mejor para**: Sitios con JavaScript din√°mico

```python
from selenium import webdriver
from selenium.webdriver.common.by import By

# Ejemplo
driver = webdriver.Chrome()
driver.get(url)
content = driver.find_element(By.TAG_NAME, 'body').text
driver.quit()
```

**Ventajas:**
- ‚úÖ Ejecuta JavaScript
- ‚úÖ Simula navegador real
- ‚úÖ Puede interactuar con elementos (clicks, forms)
- ‚úÖ Ideal para SPAs (Single Page Applications)

**Desventajas:**
- ‚ùå M√°s lento (navegador completo)
- ‚ùå Requiere driver del navegador
- ‚ùå Mayor consumo de recursos

**Instalaci√≥n:**
```bash
pip3 install selenium webdriver-manager
```

---

### 4. **Playwright** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Mejor para**: Automatizaci√≥n moderna de navegadores

```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch()
    page = browser.new_page()
    page.goto(url)
    content = page.content()
    browser.close()
```

**Ventajas:**
- ‚úÖ M√°s r√°pido que Selenium
- ‚úÖ Soporta m√∫ltiples navegadores
- ‚úÖ API moderna y limpia
- ‚úÖ Ejecuta JavaScript

**Desventajas:**
- ‚ùå Requiere instalaci√≥n de navegadores
- ‚ùå Mayor complejidad

**Instalaci√≥n:**
```bash
pip3 install playwright
playwright install chromium
```

---

### 5. **Requests-HTML** ‚≠ê‚≠ê‚≠ê‚≠ê
**Mejor para**: Balance entre simplicidad y JavaScript

```python
from requests_html import HTMLSession

session = HTMLSession()
r = session.get(url)
r.html.render()  # Ejecuta JavaScript
text = r.html.text
```

**Ventajas:**
- ‚úÖ Sintaxis simple como requests
- ‚úÖ Puede ejecutar JavaScript
- ‚úÖ Parsing integrado
- ‚úÖ F√°cil de usar

**Desventajas:**
- ‚ùå Requiere Chromium instalado
- ‚ùå Menos control que Selenium/Playwright

**Instalaci√≥n:**
```bash
pip3 install requests-html
```

---

### 6. **Trafilatura** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Mejor para**: Extracci√≥n de contenido principal

```python
import trafilatura

downloaded = trafilatura.fetch_url(url)
text = trafilatura.extract(downloaded)
```

**Ventajas:**
- ‚úÖ Extrae solo contenido relevante (sin ads, men√∫s, etc.)
- ‚úÖ Muy r√°pido
- ‚úÖ Detecta idioma autom√°ticamente
- ‚úÖ Ideal para art√≠culos y documentaci√≥n

**Desventajas:**
- ‚ùå No ejecuta JavaScript
- ‚ùå Menos control sobre el parsing

**Instalaci√≥n:**
```bash
pip3 install trafilatura
```

---

### 7. **DuckDuckGo Search** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Mejor para**: B√∫squedas en internet sin API key

```python
from duckduckgo_search import DDGS

results = DDGS().text("mulesoft patch 4.5.0", max_results=5)
for r in results:
    print(r['title'], r['href'], r['body'])
```

**Ventajas:**
- ‚úÖ No requiere API key
- ‚úÖ Sin l√≠mites de rate
- ‚úÖ Resultados de b√∫squeda instant√°neos
- ‚úÖ Muy f√°cil de usar

**Desventajas:**
- ‚ùå Solo b√∫squedas, no scraping
- ‚ùå Resultados limitados

**Instalaci√≥n:**
```bash
pip3 install duckduckgo-search
```

---

### 8. **Newspaper3k** ‚≠ê‚≠ê‚≠ê‚≠ê
**Mejor para**: Extracci√≥n de art√≠culos de noticias

```python
from newspaper import Article

article = Article(url)
article.download()
article.parse()
print(article.title)
print(article.text)
```

**Ventajas:**
- ‚úÖ Especializado en art√≠culos
- ‚úÖ Extrae metadatos (autor, fecha, etc.)
- ‚úÖ Limpia el contenido autom√°ticamente

**Desventajas:**
- ‚ùå Limitado a art√≠culos/noticias
- ‚ùå No ejecuta JavaScript

**Instalaci√≥n:**
```bash
pip3 install newspaper3k
```

---

## üéØ Recomendaci√≥n para tu Caso

### **Estrategia H√≠brida Recomendada**

```python
# 1. DuckDuckGo para b√∫squeda inicial
# 2. Trafilatura para extracci√≥n de contenido
# 3. Beautiful Soup como fallback
```

### Justificaci√≥n

1. **DuckDuckGo Search**: 
   - Sin API key
   - Encuentra URLs relevantes r√°pidamente
   - Ideal para "buscar parches de mulesoft"

2. **Trafilatura**:
   - Extrae contenido limpio
   - Perfecto para documentaci√≥n t√©cnica
   - Muy r√°pido

3. **Beautiful Soup**:
   - Fallback si Trafilatura falla
   - Mayor control sobre parsing
   - Muy confiable

---

## üìã Arquitectura de la Herramienta

```
tool_web_crawler
‚îú‚îÄ‚îÄ Fase 1: B√∫squeda (DuckDuckGo)
‚îÇ   ‚îî‚îÄ‚îÄ Obtener URLs relevantes
‚îÇ
‚îú‚îÄ‚îÄ Fase 2: Extracci√≥n (Trafilatura)
‚îÇ   ‚îî‚îÄ‚îÄ Extraer contenido de cada URL
‚îÇ
‚îú‚îÄ‚îÄ Fase 3: Procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ Limpiar texto
‚îÇ   ‚îú‚îÄ‚îÄ Resumir si es muy largo
‚îÇ   ‚îî‚îÄ‚îÄ Formatear para el LLM
‚îÇ
‚îî‚îÄ‚îÄ Fase 4: Retorno
    ‚îî‚îÄ‚îÄ Devolver contenido estructurado
```

---

## üîß Implementaci√≥n Propuesta

### Estructura del Archivo

```python
# src/tools/tool_web_crawler.py

class WebCrawlerTool:
    """
    Herramienta para buscar informaci√≥n en internet
    """
    
    def __init__(self, max_results=3, max_content_length=2000):
        self.max_results = max_results
        self.max_content_length = max_content_length
    
    def search_and_extract(self, query: str) -> dict:
        """
        Busca en internet y extrae contenido relevante
        
        Args:
            query: Consulta de b√∫squeda
            
        Returns:
            dict con resultados estructurados
        """
        # 1. Buscar URLs
        urls = self._search_duckduckgo(query)
        
        # 2. Extraer contenido
        results = []
        for url in urls[:self.max_results]:
            content = self._extract_content(url)
            if content:
                results.append(content)
        
        # 3. Formatear resultados
        return self._format_results(results)
    
    def _search_duckduckgo(self, query: str) -> list:
        """Busca URLs usando DuckDuckGo"""
        from duckduckgo_search import DDGS
        
        results = DDGS().text(query, max_results=self.max_results)
        return [r['href'] for r in results]
    
    def _extract_content(self, url: str) -> dict:
        """Extrae contenido de una URL"""
        try:
            # Intentar con Trafilatura primero
            import trafilatura
            downloaded = trafilatura.fetch_url(url)
            text = trafilatura.extract(downloaded)
            
            if text:
                return {
                    'url': url,
                    'content': text[:self.max_content_length],
                    'method': 'trafilatura'
                }
        except:
            pass
        
        try:
            # Fallback a Beautiful Soup
            import requests
            from bs4 import BeautifulSoup
            
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Remover scripts y estilos
            for script in soup(["script", "style"]):
                script.decompose()
            
            text = soup.get_text()
            
            return {
                'url': url,
                'content': text[:self.max_content_length],
                'method': 'beautifulsoup'
            }
        except:
            return None
    
    def _format_results(self, results: list) -> dict:
        """Formatea resultados para el LLM"""
        formatted = {
            'success': len(results) > 0,
            'results_count': len(results),
            'sources': []
        }
        
        for i, result in enumerate(results, 1):
            formatted['sources'].append({
                'number': i,
                'url': result['url'],
                'content': result['content'],
                'extraction_method': result['method']
            })
        
        return formatted
```

---

## üì¶ Dependencias Necesarias

```txt
# A√±adir a requirements.txt
duckduckgo-search>=4.0.0
trafilatura>=1.6.0
beautifulsoup4>=4.12.0
requests>=2.31.0
lxml>=4.9.0
```

---

## üîí Consideraciones de Seguridad

1. **Rate Limiting**: Limitar n√∫mero de requests por minuto
2. **Timeout**: Establecer timeout en requests (10s)
3. **User-Agent**: Usar user-agent apropiado
4. **Robots.txt**: Respetar robots.txt (opcional)
5. **Sanitizaci√≥n**: Limpiar contenido extra√≠do
6. **Validaci√≥n de URLs**: Verificar URLs antes de acceder

---

## üéØ Integraci√≥n con el Agente

### En tool_executor.py

```python
from tools.tool_web_crawler import WebCrawlerTool

class ToolType(Enum):
    SEMANTIC_SEARCH = "semantic_search"
    LEXICAL_SEARCH = "lexical_search"
    REGEX_SEARCH = "regex_search"
    GET_FILE_CONTENT = "get_file_content"
    WEB_CRAWLER = "web_crawler"  # NUEVO
```

### En system_prompt

```markdown
## web_crawler
Busca informaci√≥n en internet cuando la base de conocimiento interna no es suficiente.

**Cu√°ndo usar:**
- Informaci√≥n sobre parches o versiones recientes
- Documentaci√≥n actualizada no disponible internamente
- Soluciones a errores espec√≠ficos
- Best practices actualizadas

**Par√°metros:**
- query (requerido): Consulta de b√∫squeda espec√≠fica

**Ejemplo:**
<use_tool>
<tool_name>web_crawler</tool_name>
<parameters>
{
  "query": "mulesoft runtime 4.5.0 patch security vulnerability"
}
</parameters>
</use_tool>

**IMPORTANTE:** Usa esta herramienta solo cuando:
1. La informaci√≥n no est√° en la base de conocimiento
2. Se necesita informaci√≥n muy reciente
3. El usuario pregunta expl√≠citamente por informaci√≥n actualizada
```

---

## üìä Ejemplo de Uso

### Input del Usuario
```
"¬øHay alg√∫n parche reciente de MuleSoft 4.5.0 que solucione problemas de seguridad?"
```

### Herramienta Ejecutada
```xml
<use_tool>
<tool_name>web_crawler</tool_name>
<parameters>
{
  "query": "mulesoft 4.5.0 security patch 2024"
}
</parameters>
</use_tool>
```

### Output de la Herramienta
```json
{
  "success": true,
  "results_count": 3,
  "sources": [
    {
      "number": 1,
      "url": "https://docs.mulesoft.com/release-notes/...",
      "content": "MuleSoft Runtime 4.5.0 Patch 2 addresses critical security vulnerabilities...",
      "extraction_method": "trafilatura"
    },
    {
      "number": 2,
      "url": "https://help.mulesoft.com/...",
      "content": "Security Advisory: CVE-2024-XXXX affects MuleSoft Runtime 4.5.0...",
      "extraction_method": "trafilatura"
    }
  ]
}
```

---

## üöÄ Plan de Implementaci√≥n

### Fase 1: Implementaci√≥n B√°sica (2-3 horas)
- [ ] Crear tool_web_crawler.py
- [ ] Implementar b√∫squeda con DuckDuckGo
- [ ] Implementar extracci√≥n con Trafilatura
- [ ] A√±adir fallback con Beautiful Soup
- [ ] Testing b√°sico

### Fase 2: Integraci√≥n (1-2 horas)
- [ ] A√±adir a ToolType enum
- [ ] Integrar en tool_executor.py
- [ ] Actualizar system_prompt
- [ ] Testing de integraci√≥n

### Fase 3: Optimizaciones (1-2 horas)
- [ ] A√±adir rate limiting
- [ ] Implementar cach√© de resultados
- [ ] Mejorar formateo de resultados
- [ ] A√±adir m√©tricas

### Fase 4: Documentaci√≥n y Testing (1 hora)
- [ ] Documentar uso
- [ ] Crear ejemplos
- [ ] Testing end-to-end
- [ ] Actualizar README

---

## ‚úÖ Conclusi√≥n

**Recomendaci√≥n Final:**
- **DuckDuckGo Search**: Para b√∫squedas (sin API key)
- **Trafilatura**: Para extracci√≥n de contenido
- **Beautiful Soup**: Como fallback

Esta combinaci√≥n proporciona:
- ‚úÖ Sin costos (no requiere API keys)
- ‚úÖ R√°pido y eficiente
- ‚úÖ Contenido limpio y relevante
- ‚úÖ F√°cil de mantener

¬øTe gustar√≠a que implemente esta herramienta ahora?
